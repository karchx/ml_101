{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHKDfwdKIQFo"
      },
      "source": [
        "# ML 101"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow keras"
      ],
      "metadata": {
        "id": "jwwS5s5Teboi"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The encoder and decoder layers"
      ],
      "metadata": {
        "id": "UWfb-Zczei5y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "LAx6Exfzqm_Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder layer"
      ],
      "metadata": {
        "id": "J0F_c_R8eqa1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, Dense\n",
        "\n",
        "class TransformerEncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "    super(TransformerEncoderLayer, self).__init__()\n",
        "\n",
        "    self.mha = MultiHeadAttention(num_heads, d_model)\n",
        "    self.ffn = tf.keras.Sequential([\n",
        "        Dense(dff, activation='relu'),\n",
        "        Dense(d_model)\n",
        "    ])\n",
        "\n",
        "    self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, training):\n",
        "    attn_output = self.mha(x,x,x)\n",
        "    attn_output = self.dropout1(attn_output, training=training)\n",
        "    out1 = self.layernorm1(x + attn_output)\n",
        "\n",
        "    ffn_output = self.ffn(out1)\n",
        "    ffn_output = self.dropout2(ffn_output, training=training)\n",
        "    out2 = self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "    return out2"
      ],
      "metadata": {
        "id": "YityZdUnet7b"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decoder Layer"
      ],
      "metadata": {
        "id": "T4cTZe2Zqn9H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "    super(TransformerDecoderLayer, self).__init__()\n",
        "    self.mha1 = MultiHeadAttention(num_heads, d_model)\n",
        "    self.mha2 = MultiHeadAttention(num_heads, d_model)\n",
        "\n",
        "    self.ffn = tf.keras.Sequential([\n",
        "        Dense(dff, activation='relu'),\n",
        "        Dense(d_model)\n",
        "    ])\n",
        "\n",
        "    self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm3 = LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "    attn1, attn_weights_block1 = self.mha1(x,x,x,look_ahead_mask)\n",
        "    attn1 = self.dropout1(attn1, training=training)\n",
        "    out1 = self.layernorm1(attn1 + x)\n",
        "\n",
        "    attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)\n",
        "    attn2 = self.dropout2(attn2, training=training)\n",
        "    out2 = self.layernorm2(attn2 + out1)\n",
        "\n",
        "    ffn_output = self.ffn(out2)\n",
        "    ffn_output = self.dropout3(ffn_output, training=training)\n",
        "    out3 = self.layernorm3(ffn_output + out2)\n",
        "\n",
        "    return out3, attn_weights_block1, attn_weights_block2"
      ],
      "metadata": {
        "id": "4EkU8eZ6qrgV"
      },
      "execution_count": 5,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}